{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### ### The University of Melbourne, School of Computing and Information Systems\n",
    "# COMP30027 Machine Learning, 2021 Semester 1\n",
    "\n",
    "## Assignment 1: Pose classification with naive Bayes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Student ID(s):** \n",
    "Chuen Ley Low: 1078133\n",
    "Hoan Tran: 1079602\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This iPython notebook is a template which you will use for your Assignment 1 submission.\n",
    "\n",
    "Marking will be applied on the four functions that are defined in this notebook, and to your responses to the questions at the end of this notebook (Submitted in a separate PDF file).\n",
    "\n",
    "**NOTE: YOU SHOULD ADD YOUR RESULTS, DIAGRAMS AND IMAGES FROM YOUR OBSERVATIONS IN THIS FILE TO YOUR REPORT (the PDF file).**\n",
    "\n",
    "You may change the prototypes of these functions, and you may write other functions, according to your requirements. We would appreciate it if the required functions were prominent/easy to find.\n",
    "\n",
    "**Adding proper comments to your code is MANDATORY. **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'statsmodels'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-8b4b7c96eb4d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mscipy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstats\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mstats\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mstatsmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapi\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msm\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mrandom\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'statsmodels'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import pickle \n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats\n",
    "import statsmodels.api as sm\n",
    "import random\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function should prepare the data by reading it from a file and converting it into a useful format for training and testing\n",
    "\n",
    "def preprocess():\n",
    "    train_X = []\n",
    "    train_Y = []\n",
    "    test_X = []\n",
    "    test_Y = []\n",
    "    \n",
    "    train_file_path = \"data/train.csv\"\n",
    "    test_file_path = \"data/test.csv\"\n",
    "    \n",
    "    train_df = pd.read_csv(train_file_path)\n",
    "    test_df = pd.read_csv(test_file_path)\n",
    "    \n",
    "    train_X.append([float(x) for x in list(train_df.columns[1:])])\n",
    "    train_Y.append(train_df.columns[0])\n",
    "    \n",
    "    # Reads through the training data and converts instances into arrays of attributes and classes\n",
    "    for row in train_df.iterrows():\n",
    "        temp = []\n",
    "        for e in row[1][1:]:\n",
    "            temp.append(e)\n",
    "            \n",
    "        train_X.append(temp)\n",
    "        train_Y.append(row[1][0])\n",
    "    \n",
    "    test_X.append([float(x) for x in list(test_df.columns[1:])])\n",
    "    test_Y.append(test_df.columns[0])\n",
    "    \n",
    "    # Reads through the test data and converts instances into arrays of attributes and classes\n",
    "    for row in test_df.iterrows():\n",
    "        temp = []\n",
    "        for e in row[1][1:]:\n",
    "            temp.append(e)\n",
    "        \n",
    "        test_X.append(temp)\n",
    "        test_Y.append(row[1][0])\n",
    "    \n",
    "    return train_X, train_Y, test_X, test_Y\n",
    "\n",
    "train_X, train_Y, test_X, test_Y = preprocess()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function should calculate prior probabilities and likelihoods from the training data and using\n",
    "# them to build a naive Bayes model\n",
    "\n",
    "def train(train_X, train_Y):\n",
    "    label_prob_dict = {}\n",
    "    groups = {}\n",
    "    train_dict = {}\n",
    "    MISSING_POINT = 9999.0\n",
    "    \n",
    "    # Converts the arrays of classes and attributes from the training data into a dictionary\n",
    "    for data, label in zip(train_X, train_Y):\n",
    "        if label not in groups:\n",
    "            groups[label] = []\n",
    "            train_dict[label] = []\n",
    "        groups[label].append(data)\n",
    "    \n",
    "    for key in groups.keys():\n",
    "        data = groups[key]\n",
    "        num_attributes = len(data[0])\n",
    "        num_instance = len(data)\n",
    "        \n",
    "        # Calculates and stores the prior probabilities for each class\n",
    "        label_prob_dict[key] = num_instance / len(train_X)\n",
    "        \n",
    "        for i in range(num_attributes):\n",
    "            total_sum = 0 \n",
    "            diff_sum = 0\n",
    "            \n",
    "            # Calculates the mean for each attribute\n",
    "            for j in range(num_instance):\n",
    "                if math.floor(data[j][i]) != MISSING_POINT:\n",
    "                    total_sum += float(data[j][i]) \n",
    "            \n",
    "            mean = total_sum / num_instance\n",
    "            \n",
    "            # Calculates the standard deviation for each attribute\n",
    "            for j in range(num_instance):\n",
    "                if math.floor(data[j][i]) != MISSING_POINT:\n",
    "                    diff_sum += ((float(data[j][i]) - mean) ** 2)\n",
    "                \n",
    "            std_dev = math.sqrt(diff_sum / num_instance)\n",
    "            \n",
    "            train_dict[key].append((mean, std_dev))\n",
    "            \n",
    "    return train_dict, label_prob_dict, groups\n",
    "\n",
    "train_dict, label_prob_dict, group_data = train(train_X, train_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function takes in an x value, mean, and standard deviation uses the pdf of the gaussian  \n",
    "# distribution to return the probability of the x value\n",
    "\n",
    "def GaussianFunction(x, mean, std_dev):\n",
    "    x = float(x)\n",
    "    base = 1 / (std_dev * math.sqrt(2 * math.pi))\n",
    "    exponent = math.exp(-0.5 * ((x - mean)/std_dev) ** 2)\n",
    "    return base * exponent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function should predict classes for new items in a test dataset (for the purposes of this assignment, you\n",
    "# can re-use the training data as a test set)\n",
    "\n",
    "def predict(test_X, train_dict, label_prob_dict):\n",
    "    predictions = []\n",
    "    MISSING_POINT = 9999.0\n",
    "    \n",
    "    for i in range(len(test_X)):\n",
    "        instance = test_X[i]\n",
    "        prob_arr = []\n",
    "        \n",
    "        # Calculate the Gaussian probability for each label\n",
    "        for label in train_dict.keys():        \n",
    "            prob = math.log(label_prob_dict[label], 2)\n",
    "            \n",
    "            for j in range(len(instance)):\n",
    "                if math.floor(instance[j]) != MISSING_POINT:\n",
    "                    mean, std_dev = train_dict[label][j]\n",
    "                    gaussian_prob = GaussianFunction(instance[j], mean, std_dev)\n",
    "                    \n",
    "                    if gaussian_prob != 0.0:\n",
    "                        prob += math.log(GaussianFunction(instance[j], mean, std_dev), 2)\n",
    "            prob_arr.append(prob)\n",
    "        \n",
    "        max_arg = np.argmax(prob_arr)\n",
    "        predicted_label = list(train_dict.keys())[max_arg]\n",
    "        predictions.append(predicted_label)\n",
    "    \n",
    "    # Writes the predictions into a csv file\n",
    "    pd.DataFrame(predictions).to_csv(\"predictions.csv\", index = False, header = False)\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "predictions = predict(test_X, train_dict, label_prob_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7413793103448276"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This function should evaluate the prediction performance by comparing your model’s class outputs to ground\n",
    "# truth labels\n",
    "\n",
    "def evaluate(test_Y, predictions):\n",
    "    total_correct = 0\n",
    "    \n",
    "    # Compares all the predicted labels with the truth labels to calculate accuracy\n",
    "    for i in range(len(test_Y)):\n",
    "        if predictions[i] == test_Y[i]:\n",
    "            total_correct += 1\n",
    "    \n",
    "    accuracy = total_correct / len(test_Y)\n",
    "    \n",
    "    return accuracy\n",
    "    \n",
    "evaluate(test_Y, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions \n",
    "\n",
    "\n",
    "If you are in a group of 1, you will respond to **two** questions of your choosing.\n",
    "\n",
    "If you are in a group of 2, you will respond to **four** questions of your choosing.\n",
    "\n",
    "A response to a question should take about 150–250 words, and make reference to the data wherever possible.\n",
    "\n",
    "#### NOTE: you may develope codes or functions to help respond to the question here, but your formal answer should be submitted separately as a PDF."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1\n",
    "Since this is a multiclass classification problem, there are multiple ways to compute precision, recall, and F-score for this classifier. Implement at least two of the methods from the \"Model Evaluation\" lecture and discuss any differences between them. (The implementation should be your own and should not just call a pre-existing function.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Micro Average Recall: 0.7414\n",
      "Micro Average Precision: 0.7414\n",
      "Micro Average F1 Score: 0.7414\n",
      "\n",
      "\n",
      "Macro Average Recall: 0.7416\n",
      "Macro Average Precision: 0.7137\n",
      "Macro Average F1 Score: 0.7274\n",
      "\n",
      "\n",
      "Weighted Average Recall: 0.7414\n",
      "Weighted Average Precision: 0.7571\n",
      "Weighted Average F1 Score: 0.7491\n",
      "\n",
      "\n",
      "Class Proportions\n",
      "bridge: 0.1207\n",
      "childs: 0.1121\n",
      "downwarddog: 0.1552\n",
      "mountain: 0.2586\n",
      "plank: 0.0776\n",
      "seatedforwardbend: 0.0776\n",
      "tree: 0.0517\n",
      "trianglepose: 0.0345\n",
      "warrior1: 0.0431\n",
      "warrior2: 0.0690\n"
     ]
    }
   ],
   "source": [
    "true_positives = {}\n",
    "true_negatives = {}\n",
    "false_positives = {}\n",
    "false_negatives = {}\n",
    "class_proportions = {}\n",
    "    \n",
    "for key in train_dict.keys():\n",
    "    true_positives[key] = 0\n",
    "    true_negatives[key] = 0\n",
    "    false_positives[key] = 0\n",
    "    false_negatives[key] = 0\n",
    "    class_proportions[key] = 0\n",
    "\n",
    "\n",
    "for i in range(len(test_Y)):\n",
    "        \n",
    "    # When predictions match actual\n",
    "    if predictions[i] == test_Y[i]:\n",
    "        true_positives[predictions[i]] += 1\n",
    "        for key in train_dict.keys():\n",
    "            if key != predictions[i]:\n",
    "                true_negatives[key] += 1\n",
    "                    \n",
    "    # When prediction does not match\n",
    "    else:\n",
    "        false_positives[predictions[i]] += 1\n",
    "        false_negatives[test_Y[i]] += 1\n",
    "        for key in train_dict.keys():\n",
    "            if key != predictions[i] and key != test_Y[i]:\n",
    "                true_negatives[key] += 1\n",
    "    \n",
    "    \n",
    "# Micro Averaging \n",
    "    \n",
    "# Recall And Precision\n",
    "total_true_positives = 0\n",
    "total_false_neg = 0\n",
    "total_false_pos = 0\n",
    "for key in true_positives:\n",
    "    total_true_positives += true_positives[key]\n",
    "    total_false_neg += false_negatives[key]\n",
    "    total_false_pos += false_positives[key]\n",
    "    \n",
    "recall = total_true_positives / (total_true_positives + total_false_neg)\n",
    "precision = total_true_positives / (total_true_positives + total_false_pos)\n",
    "f1_score = 2 * ((precision * recall) / (precision + recall))\n",
    "\n",
    "print(f\"Micro Average Recall: {recall:.4f}\")\n",
    "print(f\"Micro Average Precision: {precision:.4f}\")\n",
    "print(f\"Micro Average F1 Score: {f1_score:.4f}\")\n",
    "    \n",
    "    \n",
    "# Macro Averaging\n",
    "    \n",
    "# Get the recall and precision for each class\n",
    "class_recall = {}\n",
    "class_precision = {}\n",
    "recall = 0\n",
    "precision = 0\n",
    "\n",
    "for key in true_positives.keys():\n",
    "    class_recall[key] = true_positives[key] / (true_positives[key] + false_negatives[key])\n",
    "    class_precision[key] = true_positives[key] / (true_positives[key] + false_positives[key])\n",
    "    \n",
    "    recall += class_recall[key]\n",
    "    precision += class_precision[key]\n",
    "    \n",
    "recall /= len(class_recall)\n",
    "precision /= len(class_precision)\n",
    "f1_score = 2 * ((precision * recall) / (precision + recall))\n",
    "\n",
    "print(\"\\n\")\n",
    "print(f\"Macro Average Recall: {recall:.4f}\")\n",
    "print(f\"Macro Average Precision: {precision:.4f}\")\n",
    "print(f\"Macro Average F1 Score: {f1_score:.4f}\")\n",
    "    \n",
    "    \n",
    "# Weight Averaging\n",
    "    \n",
    "# Get class proportions\n",
    "for instance in test_Y:\n",
    "    if instance in class_proportions:\n",
    "        class_proportions[instance] += 1\n",
    "    else:\n",
    "        class_proportions[instance] = 1\n",
    "    \n",
    "recall = 0\n",
    "precision = 0\n",
    "for key in class_proportions:\n",
    "    class_proportions[key] /= len(test_Y)\n",
    "        \n",
    "    recall += (class_proportions[key]) * (class_recall[key])\n",
    "    precision += (class_proportions[key]) * (class_precision[key])\n",
    "\n",
    "f1_score = 2 * ((precision * recall) / (precision + recall))\n",
    "\n",
    "print(\"\\n\")\n",
    "print(f\"Weighted Average Recall: {recall:.4f}\")\n",
    "print(f\"Weighted Average Precision: {precision:.4f}\")\n",
    "print(f\"Weighted Average F1 Score: {f1_score:.4f}\")   \n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"Class Proportions\")\n",
    "for label in class_proportions:\n",
    "    print(f\"{label}: {class_proportions[label]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2\n",
    "The Gaussian naıve Bayes classifier assumes that numeric attributes come from a Gaussian distribution. Is this assumption always true for the numeric attributes in this dataset? Identify some cases where the Gaussian assumption is violated and describe any evidence (or lack thereof) that this has some effect on the classifier’s predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the the left wrist points for the classes {mountain, tree and trianglepose}\n",
    "# For Left Wrist coordinates, we extract the x and y coordinates\n",
    "\n",
    "def pose_point_summary(pose, point):\n",
    "    MISSING_POINT = 9999.0\n",
    "    MAX_POINTS = 11\n",
    "    LEFT_WRIST_X = 6\n",
    "    LEFT_WRIST_Y = 17\n",
    "    \n",
    "    points = group_data[pose]\n",
    "\n",
    "    # Plot all the instances for the point\n",
    "    x_vals = []\n",
    "    y_vals = []\n",
    "\n",
    "    for row in points:\n",
    "        if row[point] != MISSING_POINT and row[point + MAX_POINTS] != MISSING_POINT:\n",
    "            x_vals.append(row[point])\n",
    "            y_vals.append(row[point + MAX_POINTS])\n",
    "\n",
    "    for i in range(len(x_vals)):\n",
    "        plt.plot(x_vals[i], y_vals[i], 'o', markersize = 5)\n",
    "        plt.title(f'Coordinates for {pose}')\n",
    "        plt.xlabel(\"X Coordinates\")\n",
    "        plt.ylabel(\"Y Coordinates\")\n",
    "\n",
    "    plt.show()\n",
    "    plt.clf()\n",
    "\n",
    "\n",
    "    # X Coordinate\n",
    "\n",
    "    # Plot a histogram graph and Gaussian distribution for x-coord\n",
    "    plt.hist(x_vals, density = True)\n",
    "    mean_x =  train_dict[pose][point][0]\n",
    "    std_dev_x = train_dict[pose][point + MAX_POINTS][1]\n",
    "\n",
    "    xmin, xmax = plt.xlim()\n",
    "    x = np.linspace(xmin, xmax,  100)\n",
    "    p = stats.norm.pdf(x, mean_x, std_dev_x)\n",
    "    plt.plot(x, p, 'k', linewidth = 2)\n",
    "    plt.title(f'{pose}: X-coord histogram and Gaussian dist.')\n",
    "    plt.show()\n",
    "    plt.clf()\n",
    "\n",
    "    # Plot the QQ plot for the x coordinate\n",
    "    sm.qqplot(np.array(x_vals), fit = True, line = '45')\n",
    "    plt.title(f\"{pose}: X-coord QQ-plot\")\n",
    "    plt.show()\n",
    "    plt.clf()\n",
    "\n",
    "\n",
    "    # Y Coordinate\n",
    "\n",
    "    # Plot a histogram graph and Gaussian distribution for y-coord\n",
    "    plt.hist(y_vals, density = True)\n",
    "    mean_y =  train_dict[pose][point + MAX_POINTS][0]\n",
    "    std_dev_y = train_dict[pose][point + MAX_POINTS][1]\n",
    "\n",
    "    ymin, ymax = plt.xlim()\n",
    "    y = np.linspace(ymin, ymax,  100)\n",
    "    p = stats.norm.pdf(y, mean_y, std_dev_y)\n",
    "    plt.plot(y, p, 'k', linewidth = 2)\n",
    "    plt.title(f'{pose}: Y-coord histogram and Gaussian dist.')\n",
    "    plt.show()\n",
    "    plt.clf()\n",
    "\n",
    "    # Plot the QQ plot for the x coordinate\n",
    "    sm.qqplot(np.array(y_vals), fit = True, line = '45')\n",
    "    plt.title(f\"{pose}: Y-coord QQ-plot\")\n",
    "    plt.show()\n",
    "    plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in class_precision:\n",
    "    print(f\"{key}: {class_precision[key]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pose_point_summary('trianglepose', 6)\n",
    "pose_point_summary('tree', 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3\n",
    "Implement a kernel density estimate (KDE) naive Bayes classifier and compare its performance to the Gaussian naive Bayes classifier. Recall that KDE has kernel bandwidth as a free parameter -- you can choose an arbitrary value for this, but a value in the range 5-25 is recommended. Discuss any differences you observe between the Gaussian and KDE naive Bayes classifiers. (As with the Gaussian naive Bayes, this KDE naive Bayes implementation should be your own and should not just call a pre-existing function.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7413793103448276\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7758620689655172"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# KDE Implementation for calculating our probabilities\n",
    "\n",
    "def predict_with_KDE(test_X, label_prob_dict, kernal_size, group_data): \n",
    "    MISSING_POINT = 9999.0\n",
    "    \n",
    "    predictions = []\n",
    "    \n",
    "    # Iterate through our testing instances we are predicting\n",
    "    for instance in test_X:\n",
    "        prob_arr = []\n",
    "        \n",
    "        # Get the gaussian probability for each label to find argmax\n",
    "        for label in train_dict:\n",
    "            gaussian_KDE_prob = math.log(label_prob_dict[label], 2)\n",
    "            \n",
    "            for i in range(len(instance)):\n",
    "                value = instance[i]\n",
    "                \n",
    "                # If its a missing point then we ignore the instance\n",
    "                if math.floor(value) == MISSING_POINT:\n",
    "                    continue\n",
    "                    \n",
    "                total_prob = 0\n",
    "                count = 0\n",
    "                train_rows = group_data[label]\n",
    "                \n",
    "                # Treat each point as a mean of a Gaussian distribution with an arbitrary kernel size and calculate the probability \n",
    "                for train_instance in train_rows:\n",
    "                    if math.floor(train_instance[i]) != MISSING_POINT:\n",
    "                        total_prob += GaussianFunction(value, train_instance[i], std_dev = kernal_size)\n",
    "                        count += 1\n",
    "                \n",
    "                # Get the mean of the gaussian probabilities calculated\n",
    "                avg_prob = total_prob / count\n",
    "                if (avg_prob > 0):\n",
    "                    gaussian_KDE_prob += math.log(avg_prob, 2)\n",
    "            \n",
    "            # Append the probability calculated for the label\n",
    "            prob_arr.append(gaussian_KDE_prob)\n",
    "            \n",
    "        # Get the max probability and the associated label    \n",
    "        max_arg = np.argmax(prob_arr)\n",
    "        predicted_label = list(train_dict.keys())[max_arg]\n",
    "        predictions.append(predicted_label)\n",
    "    \n",
    "    pd.DataFrame(predictions).to_csv(\"KDE_predictions.csv\", index = False, header = False)\n",
    "    return predictions\n",
    "                \n",
    "KDE_predictions = predict_with_KDE(test_X, label_prob_dict, 10, group_data)  \n",
    "print(evaluate(test_Y, predictions))\n",
    "accuracy_score(test_Y, KDE_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bridge: Gaussian = 0.5714, KDE = 0.7143\n",
      "childs: Gaussian = 0.7143, KDE = 0.6429\n",
      "downwarddog: Gaussian = 0.8125, KDE = 0.8750\n",
      "mountain: Gaussian = 0.8667, KDE = 0.8667\n",
      "plank: Gaussian = 0.8333, KDE = 0.8333\n",
      "seatedforwardbend: Gaussian = 0.7143, KDE = 0.7143\n",
      "tree: Gaussian = 0.4286, KDE = 0.4286\n",
      "trianglepose: Gaussian = 0.5714, KDE = 1.0000\n",
      "warrior1: Gaussian = 0.6250, KDE = 0.5000\n",
      "warrior2: Gaussian = 1.0000, KDE = 1.0000\n"
     ]
    }
   ],
   "source": [
    "# Calculating class accuracy for KDE predictions\n",
    "# Get the recall and precision for each class\n",
    "\n",
    "true_positives = {}\n",
    "true_negatives = {}\n",
    "false_positives = {}\n",
    "false_negatives = {}\n",
    "class_proportions = {}\n",
    "    \n",
    "for key in train_dict.keys():\n",
    "    true_positives[key] = 0\n",
    "    true_negatives[key] = 0\n",
    "    false_positives[key] = 0\n",
    "    false_negatives[key] = 0\n",
    "    class_proportions[key] = 0\n",
    "\n",
    "for i in range(len(test_Y)):\n",
    "        \n",
    "    # When predictions match actual\n",
    "    if KDE_predictions[i] == test_Y[i]:\n",
    "        true_positives[predictions[i]] += 1\n",
    "        for key in train_dict.keys():\n",
    "            if key != predictions[i]:\n",
    "                true_negatives[key] += 1\n",
    "                    \n",
    "    # When prediction does not match\n",
    "    else:\n",
    "        false_positives[predictions[i]] += 1\n",
    "        false_negatives[test_Y[i]] += 1\n",
    "        for key in train_dict.keys():\n",
    "            if key != predictions[i] and key != test_Y[i]:\n",
    "                true_negatives[key] += 1\n",
    "\n",
    "class_recall = {}\n",
    "class_KDE_precision = {}\n",
    "\n",
    "for key in true_positives.keys():\n",
    "    class_recall[key] = true_positives[key] / (true_positives[key] + false_negatives[key])\n",
    "    class_KDE_precision[key] = true_positives[key] / (true_positives[key] + false_positives[key])\n",
    "    \n",
    "\n",
    "for key in class_precision:\n",
    "    print(f\"{key}: Gaussian = {class_precision[key]:.4f}, KDE = {class_KDE_precision[key]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q4\n",
    "Instead of using an arbitrary kernel bandwidth for the KDE naive Bayes classifier, use random hold-out or cross-validation to choose the kernel bandwidth. Discuss how this changes the model performance compared to using an arbitrary kernel bandwidth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7672413793103449\n",
      "0.7672413793103449\n",
      "0.7672413793103449\n",
      "0.7758620689655172\n",
      "0.7758620689655172\n",
      "0.7758620689655172\n",
      "0.7758620689655172\n",
      "0.7672413793103449\n",
      "0.7586206896551724\n",
      "0.7586206896551724\n",
      "0.7586206896551724\n",
      "0.7586206896551724\n",
      "0.75\n",
      "0.7413793103448276\n",
      "0.75\n",
      "0.75\n",
      "0.75\n",
      "0.7413793103448276\n",
      "0.7327586206896551\n",
      "0.7327586206896551\n",
      "0.7327586206896551\n",
      "[0.7672413793103449, 0.7672413793103449, 0.7672413793103449, 0.7758620689655172, 0.7758620689655172, 0.7758620689655172, 0.7758620689655172, 0.7672413793103449, 0.7586206896551724, 0.7586206896551724, 0.7586206896551724, 0.7586206896551724, 0.75, 0.7413793103448276, 0.75, 0.75, 0.75, 0.7413793103448276, 0.7327586206896551, 0.7327586206896551, 0.7327586206896551]\n",
      "Best Kernel Size: 8\n"
     ]
    }
   ],
   "source": [
    "# Performing random hold out, we use the initial split of data given to us: train.csv and test.csv\n",
    "\n",
    "def determine_best_kernel(train_X, train_Y, test_X, test_Y):\n",
    "    score_list = []\n",
    "    \n",
    "    # Calculate the accuracy score for a range of kernel sizes from 5 to 25\n",
    "    for std_dev in range(5, 26):\n",
    "        score = 0   \n",
    "        _, label_prob_dict, group_data = train(train_X, train_Y)\n",
    "        KDE_predictions = predict_with_KDE(test_X, label_prob_dict, std_dev, group_data)\n",
    "        score = evaluate(test_Y, KDE_predictions)\n",
    "        \n",
    "        # Append our accuracy scores to a list \n",
    "        print(score)\n",
    "        score_list.append(score)\n",
    "    \n",
    "    print(score_list)\n",
    "    \n",
    "    # Get the index of the best score to determine the best kernel size\n",
    "    best_kernel = np.argmax(score_list) + 5\n",
    "    return best_kernel\n",
    "        \n",
    "best_kernel_size = determine_best_kernel(train_X, train_Y, test_X, test_Y)\n",
    "print(f\"Best Kernel Size: {best_kernel_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q5\n",
    "Naive Bayes ignores missing values, but in pose recognition tasks the missing values can be informative. Missing values indicate that some part of the body was obscured and sometimes this is relevant to the pose (e.g., holding one hand behind the back). Are missing values useful for this task? Implement a method that incorporates information about missing values and demonstrate whether it changes the classification results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q6\n",
    "Engineer your own pose features from the provided keypoints. Instead of using the (x,y) positions of keypoints, you might consider the angles of the limbs or body, or the distances between pairs of keypoints. How does a naive Bayes classifier based on your engineered features compare to the classifier using (x,y) values? Please note that we are interested in explainable features for pose recognition, so simply putting the (x,y) values in a neural network or similar to get an arbitrary embedding will not receive full credit for this question. You should be able to explain the rationale behind your proposed features. Also, don't forget the conditional independence assumption of naive Bayes when proposing new features -- a large set of highly-correlated features may not work well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
